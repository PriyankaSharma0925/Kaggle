{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/priyankamukulsharma/bostonhousing-regression-part1?scriptVersionId=94143726\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"f01ce624","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-27T11:25:13.130761Z","iopub.status.busy":"2022-04-27T11:25:13.130156Z","iopub.status.idle":"2022-04-27T11:25:14.168419Z","shell.execute_reply":"2022-04-27T11:25:14.169262Z","shell.execute_reply.started":"2022-04-09T11:35:16.986149Z"},"papermill":{"duration":1.093702,"end_time":"2022-04-27T11:25:14.169579","exception":false,"start_time":"2022-04-27T11:25:13.075877","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"31a1c7f4","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:14.261154Z","iopub.status.busy":"2022-04-27T11:25:14.260588Z","iopub.status.idle":"2022-04-27T11:25:14.271403Z","shell.execute_reply":"2022-04-27T11:25:14.271868Z","shell.execute_reply.started":"2022-04-09T11:35:18.021831Z"},"papermill":{"duration":0.05685,"end_time":"2022-04-27T11:25:14.272031","exception":false,"start_time":"2022-04-27T11:25:14.215181","status":"completed"},"tags":[]},"outputs":[],"source":["%config Completer.use_jedi = False"]},{"cell_type":"code","execution_count":3,"id":"69630bf7","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:14.360397Z","iopub.status.busy":"2022-04-27T11:25:14.359828Z","iopub.status.idle":"2022-04-27T11:25:14.376477Z","shell.execute_reply":"2022-04-27T11:25:14.376999Z","shell.execute_reply.started":"2022-04-09T11:35:18.038608Z"},"papermill":{"duration":0.062234,"end_time":"2022-04-27T11:25:14.377164","exception":false,"start_time":"2022-04-27T11:25:14.31493","status":"completed"},"tags":[]},"outputs":[],"source":["dataset=pd.read_csv(\"../input/bostonhoustingmlnd/housing.csv\")"]},{"cell_type":"code","execution_count":4,"id":"fcc05664","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:14.465414Z","iopub.status.busy":"2022-04-27T11:25:14.464849Z","iopub.status.idle":"2022-04-27T11:25:14.486977Z","shell.execute_reply":"2022-04-27T11:25:14.487404Z","shell.execute_reply.started":"2022-04-09T11:35:18.066155Z"},"papermill":{"duration":0.067792,"end_time":"2022-04-27T11:25:14.487584","exception":false,"start_time":"2022-04-27T11:25:14.419792","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RM</th>\n","      <th>LSTAT</th>\n","      <th>PTRATIO</th>\n","      <th>MEDV</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.575</td>\n","      <td>4.98</td>\n","      <td>15.3</td>\n","      <td>504000.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.421</td>\n","      <td>9.14</td>\n","      <td>17.8</td>\n","      <td>453600.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7.185</td>\n","      <td>4.03</td>\n","      <td>17.8</td>\n","      <td>728700.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.998</td>\n","      <td>2.94</td>\n","      <td>18.7</td>\n","      <td>701400.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.147</td>\n","      <td>5.33</td>\n","      <td>18.7</td>\n","      <td>760200.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>484</th>\n","      <td>6.593</td>\n","      <td>9.67</td>\n","      <td>21.0</td>\n","      <td>470400.0</td>\n","    </tr>\n","    <tr>\n","      <th>485</th>\n","      <td>6.120</td>\n","      <td>9.08</td>\n","      <td>21.0</td>\n","      <td>432600.0</td>\n","    </tr>\n","    <tr>\n","      <th>486</th>\n","      <td>6.976</td>\n","      <td>5.64</td>\n","      <td>21.0</td>\n","      <td>501900.0</td>\n","    </tr>\n","    <tr>\n","      <th>487</th>\n","      <td>6.794</td>\n","      <td>6.48</td>\n","      <td>21.0</td>\n","      <td>462000.0</td>\n","    </tr>\n","    <tr>\n","      <th>488</th>\n","      <td>6.030</td>\n","      <td>7.88</td>\n","      <td>21.0</td>\n","      <td>249900.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>489 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["        RM  LSTAT  PTRATIO      MEDV\n","0    6.575   4.98     15.3  504000.0\n","1    6.421   9.14     17.8  453600.0\n","2    7.185   4.03     17.8  728700.0\n","3    6.998   2.94     18.7  701400.0\n","4    7.147   5.33     18.7  760200.0\n","..     ...    ...      ...       ...\n","484  6.593   9.67     21.0  470400.0\n","485  6.120   9.08     21.0  432600.0\n","486  6.976   5.64     21.0  501900.0\n","487  6.794   6.48     21.0  462000.0\n","488  6.030   7.88     21.0  249900.0\n","\n","[489 rows x 4 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":5,"id":"e15a2103","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:14.576959Z","iopub.status.busy":"2022-04-27T11:25:14.576387Z","iopub.status.idle":"2022-04-27T11:25:14.582998Z","shell.execute_reply":"2022-04-27T11:25:14.583471Z","shell.execute_reply.started":"2022-04-09T11:35:18.096644Z"},"papermill":{"duration":0.052583,"end_time":"2022-04-27T11:25:14.583655","exception":false,"start_time":"2022-04-27T11:25:14.531072","status":"completed"},"tags":[]},"outputs":[],"source":["corr=dataset.corr()"]},{"cell_type":"code","execution_count":6,"id":"2112d5bd","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:14.672535Z","iopub.status.busy":"2022-04-27T11:25:14.671957Z","iopub.status.idle":"2022-04-27T11:25:14.68116Z","shell.execute_reply":"2022-04-27T11:25:14.68173Z","shell.execute_reply.started":"2022-04-09T11:35:18.30062Z"},"papermill":{"duration":0.055113,"end_time":"2022-04-27T11:25:14.681896","exception":false,"start_time":"2022-04-27T11:25:14.626783","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RM</th>\n","      <th>LSTAT</th>\n","      <th>PTRATIO</th>\n","      <th>MEDV</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>RM</th>\n","      <td>1.000000</td>\n","      <td>-0.612033</td>\n","      <td>-0.304559</td>\n","      <td>0.697209</td>\n","    </tr>\n","    <tr>\n","      <th>LSTAT</th>\n","      <td>-0.612033</td>\n","      <td>1.000000</td>\n","      <td>0.360445</td>\n","      <td>-0.760670</td>\n","    </tr>\n","    <tr>\n","      <th>PTRATIO</th>\n","      <td>-0.304559</td>\n","      <td>0.360445</td>\n","      <td>1.000000</td>\n","      <td>-0.519034</td>\n","    </tr>\n","    <tr>\n","      <th>MEDV</th>\n","      <td>0.697209</td>\n","      <td>-0.760670</td>\n","      <td>-0.519034</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               RM     LSTAT   PTRATIO      MEDV\n","RM       1.000000 -0.612033 -0.304559  0.697209\n","LSTAT   -0.612033  1.000000  0.360445 -0.760670\n","PTRATIO -0.304559  0.360445  1.000000 -0.519034\n","MEDV     0.697209 -0.760670 -0.519034  1.000000"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["corr"]},{"cell_type":"code","execution_count":7,"id":"77c63922","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:14.771925Z","iopub.status.busy":"2022-04-27T11:25:14.771305Z","iopub.status.idle":"2022-04-27T11:25:15.096677Z","shell.execute_reply":"2022-04-27T11:25:15.097131Z","shell.execute_reply.started":"2022-04-09T11:35:18.551559Z"},"papermill":{"duration":0.371792,"end_time":"2022-04-27T11:25:15.097297","exception":false,"start_time":"2022-04-27T11:25:14.725505","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["<AxesSubplot:>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.figure(figsize=(8,8))\n","sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')"]},{"cell_type":"code","execution_count":8,"id":"7678149f","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:15.192417Z","iopub.status.busy":"2022-04-27T11:25:15.191835Z","iopub.status.idle":"2022-04-27T11:25:15.198329Z","shell.execute_reply":"2022-04-27T11:25:15.198814Z","shell.execute_reply.started":"2022-04-09T11:35:18.945315Z"},"papermill":{"duration":0.055874,"end_time":"2022-04-27T11:25:15.198982","exception":false,"start_time":"2022-04-27T11:25:15.143108","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["RM         0\n","LSTAT      0\n","PTRATIO    0\n","MEDV       0\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset.isnull().sum()"]},{"cell_type":"code","execution_count":9,"id":"27f1ec5c","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:15.296203Z","iopub.status.busy":"2022-04-27T11:25:15.295626Z","iopub.status.idle":"2022-04-27T11:25:15.299116Z","shell.execute_reply":"2022-04-27T11:25:15.299617Z","shell.execute_reply.started":"2022-04-09T12:30:02.859696Z"},"papermill":{"duration":0.053958,"end_time":"2022-04-27T11:25:15.299846","exception":false,"start_time":"2022-04-27T11:25:15.245888","status":"completed"},"tags":[]},"outputs":[],"source":["X=dataset.iloc[:,:-1].values\n","z=y=dataset.iloc[:,-1].values\n"]},{"cell_type":"code","execution_count":10,"id":"940d0ef3","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:15.394462Z","iopub.status.busy":"2022-04-27T11:25:15.393877Z","iopub.status.idle":"2022-04-27T11:25:15.403372Z","shell.execute_reply":"2022-04-27T11:25:15.403814Z","shell.execute_reply.started":"2022-04-09T11:39:30.143675Z"},"papermill":{"duration":0.058354,"end_time":"2022-04-27T11:25:15.403985","exception":false,"start_time":"2022-04-27T11:25:15.345631","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([ 504000.,  453600.,  728700.,  701400.,  760200.,  602700.,\n","        480900.,  569100.,  346500.,  396900.,  315000.,  396900.,\n","        455700.,  428400.,  382200.,  417900.,  485100.,  367500.,\n","        424200.,  382200.,  285600.,  411600.,  319200.,  304500.,\n","        327600.,  291900.,  348600.,  310800.,  386400.,  441000.,\n","        266700.,  304500.,  277200.,  275100.,  283500.,  396900.,\n","        420000.,  441000.,  518700.,  646800.,  732900.,  558600.,\n","        531300.,  518700.,  445200.,  405300.,  420000.,  348600.,\n","        302400.,  407400.,  413700.,  430500.,  525000.,  491400.,\n","        396900.,  743400.,  518700.,  663600.,  489300.,  411600.,\n","        392700.,  336000.,  466200.,  525000.,  693000.,  493500.,\n","        407400.,  462000.,  365400.,  438900.,  508200.,  455700.,\n","        478800.,  491400.,  506100.,  449400.,  420000.,  436800.,\n","        445200.,  426300.,  588000.,  501900.,  520800.,  480900.,\n","        501900.,  558600.,  472500.,  466200.,  495600.,  602700.,\n","        474600.,  462000.,  480900.,  525000.,  432600.,  596400.,\n","        449400.,  812700.,  919800.,  697200.,  577500.,  556500.,\n","        390600.,  405300.,  422100.,  409500.,  409500.,  428400.,\n","        415800.,  407400.,  455700.,  478800.,  394800.,  392700.,\n","        388500.,  384300.,  445200.,  403200.,  428400.,  405300.,\n","        462000.,  426300.,  430500.,  363300.,  394800.,  449400.,\n","        329700.,  340200.,  378000.,  300300.,  403200.,  411600.,\n","        483000.,  386400.,  327600.,  380100.,  365400.,  359100.,\n","        279300.,  373800.,  294000.,  302400.,  281400.,  327600.,\n","        247800.,  289800.,  327600.,  306600.,  373800.,  323400.,\n","        451500.,  411600.,  321300.,  407400.,  357000.,  327600.,\n","        275100.,  867300.,  510300.,  489300.,  567000.,  476700.,\n","        525000.,  499800.,  499800.,  468300.,  365400.,  401100.,\n","        485100.,  495600.,  474600.,  617400.,  487200.,  516600.,\n","        627900.,  781200.,  835800.,  760200.,  795900.,  682500.,\n","        554400.,  621600.,  672000.,  625800.,  732900.,  777000.,\n","        640500.,  764400.,  653100.,  611100.,  699300.,  636300.,\n","        726600.,  732900.,  690900.,  506100.,  888300., 1018500.,\n","        474600.,  512400.,  472500.,  512400.,  420000.,  455700.,\n","        405300.,  470400.,  590100.,  497700.,  525000.,  489300.,\n","        602700.,  451500.,  483000.,  560700.,  455700.,  577500.,\n","        632100.,  940800.,  789600.,  663600.,  980700.,  661500.,\n","        510300.,  665700.,  875700., 1014300.,  609000.,  504000.,\n","        527100.,  661500.,  497700.,  489300.,  462000.,  422100.,\n","        466200.,  497700.,  369600.,  388500.,  510300.,  430500.,\n","        514500.,  550200.,  512400.,  520800.,  621600.,  898800.,\n","        459900.,  438900.,  924000.,  756000.,  632100.,  709800.,\n","        905100., 1024800.,  651000.,  766500.,  478800.,  644700.,\n","        913500.,  434700.,  443100.,  529200.,  512400.,  739200.,\n","        680400.,  672000.,  697200.,  695100.,  611100.,  737100.,\n","        953400.,  743400.,  966000.,  676200.,  462000.,  422100.,\n","        487200.,  468300.,  520800.,  598500.,  783300.,  585900.,\n","        501900.,  455700.,  600600.,  569100.,  426300.,  472500.,\n","        609000.,  520800.,  462000.,  554400.,  695100.,  758100.,\n","        596400.,  701400.,  592200.,  478800.,  426300.,  338100.,\n","        464100.,  407400.,  453600.,  499800.,  340200.,  373800.,\n","        415800.,  485100.,  441000.,  499800.,  485100.,  428400.,\n","        388500.,  525000.,  516600.,  483000.,  466200.,  405300.,\n","        474600.,  415800.,  359100.,  407400.,  466200.,  434700.,\n","        443100.,  409500.,  388500.,  432600.,  399000.,  392700.,\n","        686700.,  346500.,  501900.,  655200.,  367500.,  361200.,\n","        485100.,  514500.,  558600.,  480900.,  506100.,  390600.,\n","        632100.,  382200.,  432600.,  373800.,  455700.,  476700.,\n","        474600.,  525000.,  417900.,  436800.,  352800.,  577500.,\n","        459900.,  485100.,  289800.,  289800.,  315000.,  291900.,\n","        279300.,  275100.,  214200.,  218400.,  228900.,  237300.,\n","        258300.,  184800.,  151200.,  220500.,  155400.,  214200.,\n","        241500.,  317100.,  487200.,  203700.,  289800.,  266700.,\n","        275100.,  262500.,  178500.,  105000.,  132300.,  117600.,\n","        151200.,  254100.,  174300.,  178500.,  105000.,  249900.,\n","        585900.,  361200.,  577500.,  315000.,  361200.,  375900.,\n","        342300.,  147000.,  151200.,  157500.,  218400.,  184800.,\n","        176400.,  350700.,  298200.,  436800.,  281400.,  245700.,\n","        174300.,  214200.,  228900.,  231000.,  199500.,  304500.,\n","        296100.,  338100.,  300300.,  245700.,  281400.,  201600.,\n","        182700.,  176400.,  268800.,  220500.,  359100.,  386400.,\n","        323400.,  226800.,  247800.,  312900.,  264600.,  296100.,\n","        273000.,  281400.,  319200.,  338100.,  373800.,  312900.,\n","        296100.,  266700.,  283500.,  312900.,  420000.,  344400.,\n","        371700.,  409500.,  424200.,  449400.,  417900.,  399000.,\n","        401100.,  401100.,  422100.,  417900.,  411600.,  487200.,\n","        625800.,  289800.,  279300.,  350700.,  252000.,  306600.,\n","        449400.,  483000.,  497700.,  525000.,  457800.,  432600.,\n","        445200.,  401100.,  432600.,  319200.,  147000.,  170100.,\n","        285600.,  422100.,  457800.,  514500.,  485100.,  413700.,\n","        384300.,  445200.,  367500.,  352800.,  470400.,  432600.,\n","        501900.,  462000.,  249900.])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":11,"id":"e53f5643","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-04-27T11:25:15.500798Z","iopub.status.busy":"2022-04-27T11:25:15.500125Z","iopub.status.idle":"2022-04-27T11:25:15.677853Z","shell.execute_reply":"2022-04-27T11:25:15.6783Z","shell.execute_reply.started":"2022-04-09T11:35:19.269083Z"},"papermill":{"duration":0.228481,"end_time":"2022-04-27T11:25:15.678479","exception":false,"start_time":"2022-04-27T11:25:15.449998","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n","\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n"]},{"cell_type":"code","execution_count":12,"id":"a00fc991","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:15.773733Z","iopub.status.busy":"2022-04-27T11:25:15.773158Z","iopub.status.idle":"2022-04-27T11:25:15.775839Z","shell.execute_reply":"2022-04-27T11:25:15.776273Z","shell.execute_reply.started":"2022-04-09T11:35:19.489432Z"},"papermill":{"duration":0.051696,"end_time":"2022-04-27T11:25:15.776427","exception":false,"start_time":"2022-04-27T11:25:15.724731","status":"completed"},"tags":[]},"outputs":[],"source":["model_name=[]\n","mean_error=[]"]},{"cell_type":"code","execution_count":13,"id":"1ba87449","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:15.873367Z","iopub.status.busy":"2022-04-27T11:25:15.872774Z","iopub.status.idle":"2022-04-27T11:25:15.981117Z","shell.execute_reply":"2022-04-27T11:25:15.98059Z","shell.execute_reply.started":"2022-04-09T11:35:19.714743Z"},"papermill":{"duration":0.157218,"end_time":"2022-04-27T11:25:15.981253","exception":false,"start_time":"2022-04-27T11:25:15.824035","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([-3.16996173e+09, -8.49118414e+09, -1.67135644e+10, -1.31961396e+10,\n","        -1.05010583e+10]),\n"," -10414381637.151714,\n"," ['lr'],\n"," [-10414381637.151714])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.linear_model import LinearRegression,Lasso,Ridge\n","\n","lr=LinearRegression()\n","\n","cvs_lr=cross_val_score(lr,X,y,cv=5,scoring='neg_mean_squared_error')\n","model_name.append('lr')\n","mean_error.append(np.mean(cvs_lr))\n","cvs_lr,np.mean(cvs_lr),model_name,mean_error"]},{"cell_type":"code","execution_count":14,"id":"fd7601e4","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.080268Z","iopub.status.busy":"2022-04-27T11:25:16.079348Z","iopub.status.idle":"2022-04-27T11:25:16.101051Z","shell.execute_reply":"2022-04-27T11:25:16.10155Z","shell.execute_reply.started":"2022-04-09T11:35:20.098541Z"},"papermill":{"duration":0.07228,"end_time":"2022-04-27T11:25:16.101735","exception":false,"start_time":"2022-04-27T11:25:16.029455","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([-3.18223098e+09, -8.51454305e+09, -1.67792197e+10, -1.30755486e+10,\n","        -1.04194432e+10]),\n"," -10394197112.37843,\n"," ['lr', 'ridge'],\n"," [-10414381637.151714, -10394197112.37843])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#trying with ridge now\n","\n","ridge=Ridge()\n","cvs_ridge=cross_val_score(ridge,X,y,cv=5,scoring='neg_mean_squared_error')\n","model_name.append('ridge')\n","mean_error.append(np.mean(cvs_ridge))\n","cvs_ridge,np.mean(cvs_ridge),model_name,mean_error"]},{"cell_type":"code","execution_count":15,"id":"32da7789","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.199187Z","iopub.status.busy":"2022-04-27T11:25:16.198251Z","iopub.status.idle":"2022-04-27T11:25:16.213329Z","shell.execute_reply":"2022-04-27T11:25:16.212731Z","shell.execute_reply.started":"2022-04-09T11:35:20.594279Z"},"papermill":{"duration":0.064192,"end_time":"2022-04-27T11:25:16.213476","exception":false,"start_time":"2022-04-27T11:25:16.149284","status":"completed"},"tags":[]},"outputs":[],"source":["#trying with lasso now\n","lasso=Lasso()\n","cvs_lasso=cross_val_score(lasso,X,y,cv=5,scoring='neg_mean_squared_error')\n","model_name.append('lasso')\n","mean_error.append(np.mean(cvs_lasso))"]},{"cell_type":"code","execution_count":16,"id":"fabab1c4","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.312142Z","iopub.status.busy":"2022-04-27T11:25:16.309438Z","iopub.status.idle":"2022-04-27T11:25:16.314753Z","shell.execute_reply":"2022-04-27T11:25:16.315227Z","shell.execute_reply.started":"2022-04-09T11:35:20.962792Z"},"papermill":{"duration":0.055156,"end_time":"2022-04-27T11:25:16.3154","exception":false,"start_time":"2022-04-27T11:25:16.260244","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([-3.18223098e+09, -8.51454305e+09, -1.67792197e+10, -1.30755486e+10,\n","        -1.04194432e+10]),\n"," -10394197112.37843,\n"," ['lr', 'ridge', 'lasso'],\n"," [-10414381637.151714, -10394197112.37843, -10414464963.982954])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["cvs_ridge,np.mean(cvs_ridge),model_name,mean_error"]},{"cell_type":"code","execution_count":17,"id":"65882374","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.41361Z","iopub.status.busy":"2022-04-27T11:25:16.412924Z","iopub.status.idle":"2022-04-27T11:25:16.424036Z","shell.execute_reply":"2022-04-27T11:25:16.423576Z","shell.execute_reply.started":"2022-04-09T11:35:21.145675Z"},"papermill":{"duration":0.060983,"end_time":"2022-04-27T11:25:16.424173","exception":false,"start_time":"2022-04-27T11:25:16.36319","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model Name</th>\n","      <th>Mean_error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>lr</td>\n","      <td>-10414381637.151714</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ridge</td>\n","      <td>-10394197112.378429</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>lasso</td>\n","      <td>-10414464963.982954</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Model Name          Mean_error\n","0         lr -10414381637.151714\n","1      ridge -10394197112.378429\n","2      lasso -10414464963.982954"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["ScoreTable=pd.DataFrame(data=[model_name,mean_error]).T\n","ScoreTable.rename(columns={0:\"Model Name\",1:\"Mean_error\"})\n"]},{"cell_type":"code","execution_count":18,"id":"e828a6ee","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.523964Z","iopub.status.busy":"2022-04-27T11:25:16.523194Z","iopub.status.idle":"2022-04-27T11:25:16.525868Z","shell.execute_reply":"2022-04-27T11:25:16.525388Z","shell.execute_reply.started":"2022-04-09T11:35:21.377716Z"},"papermill":{"duration":0.054649,"end_time":"2022-04-27T11:25:16.526001","exception":false,"start_time":"2022-04-27T11:25:16.471352","status":"completed"},"tags":[]},"outputs":[],"source":["# now using gridSearchCV for finding the best regularization parameter\n","\n","parameters={'alpha':[0.001,0.01,0.1,1,10,100]}\n","cv_ridge=GridSearchCV(ridge,param_grid=parameters,scoring='neg_mean_squared_error')"]},{"cell_type":"code","execution_count":19,"id":"dbf26f5d","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.629428Z","iopub.status.busy":"2022-04-27T11:25:16.628652Z","iopub.status.idle":"2022-04-27T11:25:16.670068Z","shell.execute_reply":"2022-04-27T11:25:16.670539Z","shell.execute_reply.started":"2022-04-09T11:35:21.607896Z"},"papermill":{"duration":0.097336,"end_time":"2022-04-27T11:25:16.670707","exception":false,"start_time":"2022-04-27T11:25:16.573371","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["GridSearchCV(estimator=Ridge(),\n","             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n","             scoring='neg_mean_squared_error')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["cv_ridge.fit(X,y)"]},{"cell_type":"code","execution_count":20,"id":"a1b3de51","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.768696Z","iopub.status.busy":"2022-04-27T11:25:16.768104Z","iopub.status.idle":"2022-04-27T11:25:16.770826Z","shell.execute_reply":"2022-04-27T11:25:16.771255Z","shell.execute_reply.started":"2022-04-09T11:35:21.75116Z"},"papermill":{"duration":0.053401,"end_time":"2022-04-27T11:25:16.771412","exception":false,"start_time":"2022-04-27T11:25:16.718011","status":"completed"},"tags":[]},"outputs":[],"source":["Ridge_details=[]\n","lasso_details=[]"]},{"cell_type":"code","execution_count":21,"id":"48ffd6be","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.872165Z","iopub.status.busy":"2022-04-27T11:25:16.871235Z","iopub.status.idle":"2022-04-27T11:25:16.873351Z","shell.execute_reply":"2022-04-27T11:25:16.87382Z","shell.execute_reply.started":"2022-04-09T11:35:21.977029Z"},"papermill":{"duration":0.054232,"end_time":"2022-04-27T11:25:16.873987","exception":false,"start_time":"2022-04-27T11:25:16.819755","status":"completed"},"tags":[]},"outputs":[],"source":["\n","Ridge_details.append(cv_ridge.best_estimator_)\n","Ridge_details.append(cv_ridge.best_score_)"]},{"cell_type":"code","execution_count":22,"id":"0c7f3499","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:16.972429Z","iopub.status.busy":"2022-04-27T11:25:16.97154Z","iopub.status.idle":"2022-04-27T11:25:16.97671Z","shell.execute_reply":"2022-04-27T11:25:16.977189Z","shell.execute_reply.started":"2022-04-09T11:35:22.233267Z"},"papermill":{"duration":0.055953,"end_time":"2022-04-27T11:25:16.977352","exception":false,"start_time":"2022-04-27T11:25:16.921399","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[Ridge(alpha=100), -10146819505.51907]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["Ridge_details"]},{"cell_type":"code","execution_count":23,"id":"f841b71a","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.07837Z","iopub.status.busy":"2022-04-27T11:25:17.077409Z","iopub.status.idle":"2022-04-27T11:25:17.079544Z","shell.execute_reply":"2022-04-27T11:25:17.080048Z","shell.execute_reply.started":"2022-04-09T11:35:22.422082Z"},"papermill":{"duration":0.054963,"end_time":"2022-04-27T11:25:17.080222","exception":false,"start_time":"2022-04-27T11:25:17.025259","status":"completed"},"tags":[]},"outputs":[],"source":["parameters={'alpha':[0.001,0.01,0.1,1,10,100]}\n","cv_lasso=GridSearchCV(lasso,param_grid=parameters,scoring='neg_mean_squared_error')"]},{"cell_type":"code","execution_count":24,"id":"978a77cd","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.180918Z","iopub.status.busy":"2022-04-27T11:25:17.180015Z","iopub.status.idle":"2022-04-27T11:25:17.22759Z","shell.execute_reply":"2022-04-27T11:25:17.228051Z","shell.execute_reply.started":"2022-04-09T11:35:22.575725Z"},"papermill":{"duration":0.099722,"end_time":"2022-04-27T11:25:17.228236","exception":false,"start_time":"2022-04-27T11:25:17.128514","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["GridSearchCV(estimator=Lasso(),\n","             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n","             scoring='neg_mean_squared_error')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["cv_lasso.fit(X,y)"]},{"cell_type":"code","execution_count":25,"id":"8fe7892b","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.329822Z","iopub.status.busy":"2022-04-27T11:25:17.329173Z","iopub.status.idle":"2022-04-27T11:25:17.332321Z","shell.execute_reply":"2022-04-27T11:25:17.33292Z","shell.execute_reply.started":"2022-04-09T11:35:22.673253Z"},"papermill":{"duration":0.055451,"end_time":"2022-04-27T11:25:17.333087","exception":false,"start_time":"2022-04-27T11:25:17.277636","status":"completed"},"tags":[]},"outputs":[],"source":["lasso_details.append(cv_lasso.best_estimator_)\n","lasso_details.append(cv_lasso.best_score_)"]},{"cell_type":"code","execution_count":26,"id":"e120ecc5","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.432654Z","iopub.status.busy":"2022-04-27T11:25:17.43207Z","iopub.status.idle":"2022-04-27T11:25:17.437646Z","shell.execute_reply":"2022-04-27T11:25:17.438024Z","shell.execute_reply.started":"2022-04-09T11:35:22.765237Z"},"papermill":{"duration":0.056601,"end_time":"2022-04-27T11:25:17.438187","exception":false,"start_time":"2022-04-27T11:25:17.381586","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[Lasso(alpha=0.001), -10414381757.638432]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["lasso_details"]},{"cell_type":"code","execution_count":27,"id":"bc9ee4c9","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.546394Z","iopub.status.busy":"2022-04-27T11:25:17.545795Z","iopub.status.idle":"2022-04-27T11:25:17.548391Z","shell.execute_reply":"2022-04-27T11:25:17.548874Z","shell.execute_reply.started":"2022-04-09T11:35:22.919116Z"},"papermill":{"duration":0.061849,"end_time":"2022-04-27T11:25:17.549038","exception":false,"start_time":"2022-04-27T11:25:17.487189","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ridge(alpha=100)</td>\n","      <td>-1.014682e+10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lasso(alpha=0.001)</td>\n","      <td>-1.041438e+10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    0             1\n","0    Ridge(alpha=100) -1.014682e+10\n","1  Lasso(alpha=0.001) -1.041438e+10"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["GridCVResults=pd.DataFrame(data=[Ridge_details,lasso_details])\n","GridCVResults"]},{"cell_type":"code","execution_count":28,"id":"87b39a50","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.652262Z","iopub.status.busy":"2022-04-27T11:25:17.651686Z","iopub.status.idle":"2022-04-27T11:25:17.653778Z","shell.execute_reply":"2022-04-27T11:25:17.654195Z","shell.execute_reply.started":"2022-04-09T11:35:23.1045Z"},"papermill":{"duration":0.055919,"end_time":"2022-04-27T11:25:17.654352","exception":false,"start_time":"2022-04-27T11:25:17.598433","status":"completed"},"tags":[]},"outputs":[],"source":["norm_ridge=[]\n","norm_lasso=[]\n","\n","ridge_1=Ridge(normalize=True)\n","lasso_1=Lasso(normalize=True)\n","\n"]},{"cell_type":"code","execution_count":29,"id":"f68ada76","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.758885Z","iopub.status.busy":"2022-04-27T11:25:17.758268Z","iopub.status.idle":"2022-04-27T11:25:17.828627Z","shell.execute_reply":"2022-04-27T11:25:17.827725Z","shell.execute_reply.started":"2022-04-09T11:35:23.28592Z"},"papermill":{"duration":0.125843,"end_time":"2022-04-27T11:25:17.828828","exception":false,"start_time":"2022-04-27T11:25:17.702985","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n"]},{"data":{"text/plain":["GridSearchCV(estimator=Ridge(normalize=True),\n","             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n","             scoring='neg_mean_squared_error')"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["ridge_Grid=GridSearchCV(ridge_1,param_grid=parameters,scoring='neg_mean_squared_error')\n","lasso_Grid=GridSearchCV(lasso_1,param_grid=parameters,scoring='neg_mean_squared_error')\n","ridge_Grid.fit(X,y)\n"]},{"cell_type":"code","execution_count":30,"id":"3ad1be11","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:17.940199Z","iopub.status.busy":"2022-04-27T11:25:17.937203Z","iopub.status.idle":"2022-04-27T11:25:18.010203Z","shell.execute_reply":"2022-04-27T11:25:18.009549Z","shell.execute_reply.started":"2022-04-09T11:35:23.462437Z"},"papermill":{"duration":0.130429,"end_time":"2022-04-27T11:25:18.010346","exception":false,"start_time":"2022-04-27T11:25:17.879917","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n"]},{"data":{"text/plain":["GridSearchCV(estimator=Lasso(normalize=True),\n","             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n","             scoring='neg_mean_squared_error')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["lasso_Grid.fit(X,y)"]},{"cell_type":"code","execution_count":31,"id":"27cc8663","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:18.120961Z","iopub.status.busy":"2022-04-27T11:25:18.120268Z","iopub.status.idle":"2022-04-27T11:25:18.123153Z","shell.execute_reply":"2022-04-27T11:25:18.122699Z","shell.execute_reply.started":"2022-04-09T11:35:23.626383Z"},"papermill":{"duration":0.059772,"end_time":"2022-04-27T11:25:18.123285","exception":false,"start_time":"2022-04-27T11:25:18.063513","status":"completed"},"tags":[]},"outputs":[],"source":["norm_ridge.append(ridge_Grid.best_estimator_)\n","norm_ridge.append(ridge_Grid.best_score_)\n","norm_lasso.append(lasso_Grid.best_estimator_)\n","norm_lasso.append(lasso_Grid.best_score_)"]},{"cell_type":"code","execution_count":32,"id":"b8e13c5a","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:18.232221Z","iopub.status.busy":"2022-04-27T11:25:18.231574Z","iopub.status.idle":"2022-04-27T11:25:18.233935Z","shell.execute_reply":"2022-04-27T11:25:18.233363Z","shell.execute_reply.started":"2022-04-09T11:35:23.738778Z"},"papermill":{"duration":0.058241,"end_time":"2022-04-27T11:25:18.234065","exception":false,"start_time":"2022-04-27T11:25:18.175824","status":"completed"},"tags":[]},"outputs":[],"source":["norm_score=pd.DataFrame(data=[norm_ridge,norm_lasso])"]},{"cell_type":"code","execution_count":33,"id":"734d9929","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:18.340451Z","iopub.status.busy":"2022-04-27T11:25:18.339914Z","iopub.status.idle":"2022-04-27T11:25:18.348036Z","shell.execute_reply":"2022-04-27T11:25:18.348559Z","shell.execute_reply.started":"2022-04-09T11:35:23.845993Z"},"papermill":{"duration":0.06285,"end_time":"2022-04-27T11:25:18.34872","exception":false,"start_time":"2022-04-27T11:25:18.28587","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ridge(alpha=0.1, normalize=True)</td>\n","      <td>-1.030252e+10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lasso(alpha=0.001, normalize=True)</td>\n","      <td>-1.041438e+10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    0             1\n","0    Ridge(alpha=0.1, normalize=True) -1.030252e+10\n","1  Lasso(alpha=0.001, normalize=True) -1.041438e+10"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["norm_score"]},{"cell_type":"code","execution_count":34,"id":"a315fafc","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:18.455914Z","iopub.status.busy":"2022-04-27T11:25:18.455314Z","iopub.status.idle":"2022-04-27T11:25:18.477312Z","shell.execute_reply":"2022-04-27T11:25:18.477854Z","shell.execute_reply.started":"2022-04-09T11:35:23.995282Z"},"papermill":{"duration":0.076872,"end_time":"2022-04-27T11:25:18.478018","exception":false,"start_time":"2022-04-27T11:25:18.401146","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [0, 1]\n","Index: []"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["GridCVResults.merge(norm_score)"]},{"cell_type":"code","execution_count":35,"id":"a6e70271","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:18.586143Z","iopub.status.busy":"2022-04-27T11:25:18.585573Z","iopub.status.idle":"2022-04-27T11:25:18.593583Z","shell.execute_reply":"2022-04-27T11:25:18.594111Z","shell.execute_reply.started":"2022-04-09T11:35:24.101885Z"},"papermill":{"duration":0.06393,"end_time":"2022-04-27T11:25:18.594269","exception":false,"start_time":"2022-04-27T11:25:18.530339","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ridge(alpha=100)</td>\n","      <td>-1.014682e+10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lasso(alpha=0.001)</td>\n","      <td>-1.041438e+10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    0             1\n","0    Ridge(alpha=100) -1.014682e+10\n","1  Lasso(alpha=0.001) -1.041438e+10"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["GridCVResults"]},{"cell_type":"markdown","id":"ab54f180","metadata":{"papermill":{"duration":0.054207,"end_time":"2022-04-27T11:25:18.701293","exception":false,"start_time":"2022-04-27T11:25:18.647086","status":"completed"},"tags":[]},"source":["# Lasso with alpha =0.001 has the minimum error."]},{"cell_type":"markdown","id":"5774d0b0","metadata":{"papermill":{"duration":0.0529,"end_time":"2022-04-27T11:25:18.806665","exception":false,"start_time":"2022-04-27T11:25:18.753765","status":"completed"},"tags":[]},"source":["# Now trying the polynomial regression"]},{"cell_type":"code","execution_count":36,"id":"44eeb6cd","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:18.917319Z","iopub.status.busy":"2022-04-27T11:25:18.916766Z","iopub.status.idle":"2022-04-27T11:25:18.919568Z","shell.execute_reply":"2022-04-27T11:25:18.920082Z","shell.execute_reply.started":"2022-04-09T11:35:24.164061Z"},"papermill":{"duration":0.059235,"end_time":"2022-04-27T11:25:18.920243","exception":false,"start_time":"2022-04-27T11:25:18.861008","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","poly=PolynomialFeatures(degree=2)"]},{"cell_type":"code","execution_count":37,"id":"730a423b","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:19.029462Z","iopub.status.busy":"2022-04-27T11:25:19.028881Z","iopub.status.idle":"2022-04-27T11:25:19.034335Z","shell.execute_reply":"2022-04-27T11:25:19.034843Z","shell.execute_reply.started":"2022-04-09T11:35:24.225041Z"},"papermill":{"duration":0.062223,"end_time":"2022-04-27T11:25:19.035019","exception":false,"start_time":"2022-04-27T11:25:18.972796","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(489, 3)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["\n","X_poly=poly.fit_transform(X)\n","X_poly\n","X_poly.shape\n","X.shape"]},{"cell_type":"markdown","id":"bd74ed09","metadata":{"papermill":{"duration":0.05382,"end_time":"2022-04-27T11:25:19.14227","exception":false,"start_time":"2022-04-27T11:25:19.08845","status":"completed"},"tags":[]},"source":["**Apply Linear regression**"]},{"cell_type":"code","execution_count":38,"id":"3b6548d4","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:19.252781Z","iopub.status.busy":"2022-04-27T11:25:19.252146Z","iopub.status.idle":"2022-04-27T11:25:19.270115Z","shell.execute_reply":"2022-04-27T11:25:19.269557Z","shell.execute_reply.started":"2022-04-09T11:35:24.291669Z"},"papermill":{"duration":0.074133,"end_time":"2022-04-27T11:25:19.270256","exception":false,"start_time":"2022-04-27T11:25:19.196123","status":"completed"},"tags":[]},"outputs":[],"source":["cvs_lr=cross_val_score(lr,X_poly,y,cv=10,scoring='neg_mean_squared_error')"]},{"cell_type":"code","execution_count":39,"id":"f239e802","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:19.383071Z","iopub.status.busy":"2022-04-27T11:25:19.382323Z","iopub.status.idle":"2022-04-27T11:25:19.385378Z","shell.execute_reply":"2022-04-27T11:25:19.385829Z","shell.execute_reply.started":"2022-04-09T11:35:24.368608Z"},"papermill":{"duration":0.062469,"end_time":"2022-04-27T11:25:19.385995","exception":false,"start_time":"2022-04-27T11:25:19.323526","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([-2.55522907e+09, -1.85568921e+09, -5.18351618e+09, -7.15382433e+09,\n","        -6.76551224e+09, -3.82247760e+09, -4.08063861e+09, -1.24838953e+10,\n","        -6.74792307e+09, -4.51809086e+09]),\n"," -5516679648.463518)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["cvs_lr,np.mean(cvs_lr)"]},{"cell_type":"code","execution_count":40,"id":"0d6b72cf","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:19.496261Z","iopub.status.busy":"2022-04-27T11:25:19.495366Z","iopub.status.idle":"2022-04-27T11:25:19.500007Z","shell.execute_reply":"2022-04-27T11:25:19.500397Z","shell.execute_reply.started":"2022-04-09T11:35:24.445201Z"},"papermill":{"duration":0.061296,"end_time":"2022-04-27T11:25:19.500585","exception":false,"start_time":"2022-04-27T11:25:19.439289","status":"completed"},"tags":[]},"outputs":[],"source":["parameters={'alpha':[0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n","cv_lasso=GridSearchCV(lasso,param_grid=parameters,scoring='neg_mean_squared_error',cv=10)\n","cv_ridge=GridSearchCV(ridge,param_grid=parameters,scoring='neg_mean_squared_error',cv=10)"]},{"cell_type":"code","execution_count":41,"id":"6dc50c39","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:19.613555Z","iopub.status.busy":"2022-04-27T11:25:19.610689Z","iopub.status.idle":"2022-04-27T11:25:20.126638Z","shell.execute_reply":"2022-04-27T11:25:20.125939Z","shell.execute_reply.started":"2022-04-09T11:35:24.49114Z"},"papermill":{"duration":0.573104,"end_time":"2022-04-27T11:25:20.126795","exception":false,"start_time":"2022-04-27T11:25:19.553691","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.423e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.459e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.861e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.989e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.423e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.459e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.861e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.989e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.423e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.459e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.861e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.989e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.423e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.459e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.861e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.989e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.423e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.459e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.861e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.989e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.423e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.459e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.860e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.989e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.064e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.011e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.426e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.462e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.853e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.491e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.991e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+12, tolerance: 1.253e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.065e+12, tolerance: 1.286e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+12, tolerance: 1.265e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.438e+11, tolerance: 1.157e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.475e+11, tolerance: 1.124e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.017e+12, tolerance: 1.020e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.021e+12, tolerance: 1.297e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.795e+11, tolerance: 1.123e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.485e+11, tolerance: 1.134e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.996e+11, tolerance: 1.280e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+12, tolerance: 1.334e+09\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"]},{"data":{"text/plain":["GridSearchCV(cv=10, estimator=Ridge(),\n","             param_grid={'alpha': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10,\n","                                   100]},\n","             scoring='neg_mean_squared_error')"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["cv_lasso.fit(X_poly,y)\n","cv_ridge.fit(X_poly,y)\n"]},{"cell_type":"code","execution_count":42,"id":"4aa6a857","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:20.244497Z","iopub.status.busy":"2022-04-27T11:25:20.24387Z","iopub.status.idle":"2022-04-27T11:25:20.246246Z","shell.execute_reply":"2022-04-27T11:25:20.24669Z","shell.execute_reply.started":"2022-04-09T11:35:25.238485Z"},"papermill":{"duration":0.063584,"end_time":"2022-04-27T11:25:20.246865","exception":false,"start_time":"2022-04-27T11:25:20.183281","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(Ridge(alpha=1),\n"," -5358355142.443518,\n"," Lasso(alpha=100),\n"," -5381891414.948438,\n"," -5516679648.463518)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["cv_ridge.best_estimator_,cv_ridge.best_score_,cv_lasso.best_estimator_,cv_lasso.best_score_,np.mean(cvs_lr)"]},{"cell_type":"markdown","id":"34dfc478","metadata":{"papermill":{"duration":0.055756,"end_time":"2022-04-27T11:25:20.359977","exception":false,"start_time":"2022-04-27T11:25:20.304221","status":"completed"},"tags":[]},"source":["# <a id=\"1\">1. Introduction</a> <br>\n","# Support vector machine"]},{"cell_type":"code","execution_count":43,"id":"43e9a84d","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:20.482539Z","iopub.status.busy":"2022-04-27T11:25:20.481541Z","iopub.status.idle":"2022-04-27T11:25:20.484058Z","shell.execute_reply":"2022-04-27T11:25:20.483474Z","shell.execute_reply.started":"2022-04-09T11:35:25.249926Z"},"papermill":{"duration":0.062023,"end_time":"2022-04-27T11:25:20.484189","exception":false,"start_time":"2022-04-27T11:25:20.422166","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.svm import SVR"]},{"cell_type":"code","execution_count":44,"id":"4cd98ae9","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:20.599875Z","iopub.status.busy":"2022-04-27T11:25:20.598902Z","iopub.status.idle":"2022-04-27T11:25:20.601151Z","shell.execute_reply":"2022-04-27T11:25:20.601565Z","shell.execute_reply.started":"2022-04-09T11:47:43.994186Z"},"papermill":{"duration":0.062104,"end_time":"2022-04-27T11:25:20.601743","exception":false,"start_time":"2022-04-27T11:25:20.539639","status":"completed"},"tags":[]},"outputs":[],"source":["svr=SVR(kernel = 'rbf')"]},{"cell_type":"code","execution_count":45,"id":"900a5422","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:20.715307Z","iopub.status.busy":"2022-04-27T11:25:20.714412Z","iopub.status.idle":"2022-04-27T11:25:20.718063Z","shell.execute_reply":"2022-04-27T11:25:20.71858Z","shell.execute_reply.started":"2022-04-09T11:35:25.276667Z"},"papermill":{"duration":0.062101,"end_time":"2022-04-27T11:25:20.718762","exception":false,"start_time":"2022-04-27T11:25:20.656661","status":"completed"},"tags":[]},"outputs":[],"source":["#Scaling the features\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":46,"id":"44772a8a","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:20.834667Z","iopub.status.busy":"2022-04-27T11:25:20.83372Z","iopub.status.idle":"2022-04-27T11:25:20.837318Z","shell.execute_reply":"2022-04-27T11:25:20.837872Z","shell.execute_reply.started":"2022-04-09T11:35:25.287975Z"},"papermill":{"duration":0.063347,"end_time":"2022-04-27T11:25:20.838041","exception":false,"start_time":"2022-04-27T11:25:20.774694","status":"completed"},"tags":[]},"outputs":[],"source":["Sc_X=StandardScaler()\n","Sc_y=StandardScaler()"]},{"cell_type":"code","execution_count":47,"id":"f57794b2","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:20.954077Z","iopub.status.busy":"2022-04-27T11:25:20.953345Z","iopub.status.idle":"2022-04-27T11:25:20.955822Z","shell.execute_reply":"2022-04-27T11:25:20.956349Z","shell.execute_reply.started":"2022-04-09T11:43:55.800742Z"},"papermill":{"duration":0.062872,"end_time":"2022-04-27T11:25:20.956543","exception":false,"start_time":"2022-04-27T11:25:20.893671","status":"completed"},"tags":[]},"outputs":[],"source":["\n","#y.shape\n","X=Sc_X.fit_transform(X)\n","\n"]},{"cell_type":"code","execution_count":48,"id":"673e5ac0","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.071275Z","iopub.status.busy":"2022-04-27T11:25:21.070361Z","iopub.status.idle":"2022-04-27T11:25:21.075813Z","shell.execute_reply":"2022-04-27T11:25:21.076342Z","shell.execute_reply.started":"2022-04-09T12:04:09.525955Z"},"papermill":{"duration":0.064772,"end_time":"2022-04-27T11:25:21.076531","exception":false,"start_time":"2022-04-27T11:25:21.011759","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([[ 0.52055395, -1.1250769 , -1.5250831 ],\n","       [ 0.28104837, -0.53706982, -0.33974768],\n","       [ 1.46924486, -1.25935736, -0.33974768],\n","       ...,\n","       [ 1.14420158, -1.03178731,  1.17748167],\n","       [ 0.86114953, -0.91305511,  1.17748167],\n","       [-0.32704695, -0.71516812,  1.17748167]])"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"code","execution_count":49,"id":"8e7926db","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.193292Z","iopub.status.busy":"2022-04-27T11:25:21.192629Z","iopub.status.idle":"2022-04-27T11:25:21.195361Z","shell.execute_reply":"2022-04-27T11:25:21.194823Z","shell.execute_reply.started":"2022-04-09T11:46:12.40352Z"},"papermill":{"duration":0.062331,"end_time":"2022-04-27T11:25:21.195528","exception":false,"start_time":"2022-04-27T11:25:21.133197","status":"completed"},"tags":[]},"outputs":[],"source":["y=y.reshape(len(y),1)"]},{"cell_type":"code","execution_count":50,"id":"f7aee357","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.31079Z","iopub.status.busy":"2022-04-27T11:25:21.309828Z","iopub.status.idle":"2022-04-27T11:25:21.314523Z","shell.execute_reply":"2022-04-27T11:25:21.314044Z","shell.execute_reply.started":"2022-04-09T11:46:29.31457Z"},"papermill":{"duration":0.063227,"end_time":"2022-04-27T11:25:21.31467","exception":false,"start_time":"2022-04-27T11:25:21.251443","status":"completed"},"tags":[]},"outputs":[],"source":["y=Sc_y.fit_transform(y)\n"]},{"cell_type":"code","execution_count":51,"id":"b7866fe7","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.43062Z","iopub.status.busy":"2022-04-27T11:25:21.429969Z","iopub.status.idle":"2022-04-27T11:25:21.452081Z","shell.execute_reply":"2022-04-27T11:25:21.452762Z","shell.execute_reply.started":"2022-04-09T11:47:47.8883Z"},"papermill":{"duration":0.082918,"end_time":"2022-04-27T11:25:21.452989","exception":false,"start_time":"2022-04-27T11:25:21.370071","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"data":{"text/plain":["SVR()"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["svr.fit(X,y)"]},{"cell_type":"code","execution_count":52,"id":"da8b6b87","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.571048Z","iopub.status.busy":"2022-04-27T11:25:21.568886Z","iopub.status.idle":"2022-04-27T11:25:21.583296Z","shell.execute_reply":"2022-04-27T11:25:21.582795Z","shell.execute_reply.started":"2022-04-09T12:27:44.522681Z"},"papermill":{"duration":0.073486,"end_time":"2022-04-27T11:25:21.583432","exception":false,"start_time":"2022-04-27T11:25:21.509946","status":"completed"},"tags":[]},"outputs":[],"source":["y_pred=svr.predict(X)\n","svr_y=Sc_y.inverse_transform(y_pred.reshape(-1,1))"]},{"cell_type":"code","execution_count":53,"id":"31af253b","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.700047Z","iopub.status.busy":"2022-04-27T11:25:21.699171Z","iopub.status.idle":"2022-04-27T11:25:21.702112Z","shell.execute_reply":"2022-04-27T11:25:21.702572Z","shell.execute_reply.started":"2022-04-09T12:27:51.983266Z"},"papermill":{"duration":0.062807,"end_time":"2022-04-27T11:25:21.70274","exception":false,"start_time":"2022-04-27T11:25:21.639933","status":"completed"},"tags":[]},"outputs":[],"source":["mean_error=mean_error"]},{"cell_type":"code","execution_count":54,"id":"377c578d","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.825143Z","iopub.status.busy":"2022-04-27T11:25:21.817972Z","iopub.status.idle":"2022-04-27T11:25:21.827421Z","shell.execute_reply":"2022-04-27T11:25:21.827975Z","shell.execute_reply.started":"2022-04-09T12:12:38.911624Z"},"papermill":{"duration":0.069671,"end_time":"2022-04-27T11:25:21.828148","exception":false,"start_time":"2022-04-27T11:25:21.758477","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([ 7.87848611e-01,  2.37069753e-01,  1.72788968e+00,  1.35844120e+00,\n","        1.32681733e+00,  5.10935869e-01, -7.51242765e-02, -4.60155004e-01,\n","       -7.52895660e-01, -3.28504984e-01, -5.62516144e-01, -1.18626291e-01,\n","       -2.71607687e-01, -1.55495148e-01, -1.41103481e-01, -1.92967872e-01,\n","       -1.62015811e-01, -4.38206520e-01, -3.18833007e-01, -2.78692409e-01,\n","       -1.12574154e+00, -3.72469794e-01, -8.50274059e-01, -9.94988086e-01,\n","       -6.02810099e-01, -6.59194767e-01, -4.79981264e-01, -6.94877025e-01,\n","       -2.39584172e-01, -1.73847993e-01, -1.27998070e+00, -2.92645854e-01,\n","       -1.60506110e+00, -8.38292293e-01, -1.02678085e+00, -1.03482946e-01,\n","       -1.93033733e-01, -1.38379057e-01, -9.64246810e-02,  8.10396163e-01,\n","        1.54853152e+00,  1.03870324e+00,  2.03181243e-01,  1.74416455e-01,\n","       -2.06916963e-02, -2.75783814e-01, -1.74925155e-01, -2.86495111e-01,\n","       -8.20000351e-01, -1.83962495e-01, -1.35862842e-01, -7.85078822e-03,\n","        6.41088481e-01, -5.84226282e-02, -4.63264478e-01,  1.71422287e+00,\n","        4.46601725e-01,  1.24364276e+00,  7.87260882e-02, -1.10326400e-01,\n","       -3.17077278e-01, -3.52710540e-01,  3.16680244e-01,  2.59605320e-01,\n","        9.12306944e-01,  4.05703930e-01, -1.83970589e-01, -1.12305532e-01,\n","       -3.02976062e-01, -1.16405997e-01,  3.61961484e-01, -9.22213301e-02,\n","        7.58594269e-02,  1.79940017e-01,  2.65499139e-01,  1.62328680e-01,\n","       -4.97977439e-02,  3.69466526e-03, -8.60612607e-02, -1.30250276e-01,\n","        7.89833699e-01,  5.24628921e-01,  2.78350691e-01,  1.28523859e-01,\n","        1.82645844e-01,  6.54956804e-01, -1.44021864e-01,  6.54177687e-02,\n","        1.30079618e+00,  1.37475630e+00,  2.61734388e-01,  3.01353004e-01,\n","        3.41920506e-01,  2.35465097e-01,  4.15020180e-02,  6.66407483e-01,\n","       -3.43327566e-02,  2.56228276e+00,  2.47924449e+00,  1.73361472e+00,\n","        4.67602836e-02,  1.70181037e-01, -7.69401504e-02, -3.13116119e-01,\n","       -2.24646337e-01, -6.31384094e-01, -8.64730802e-01, -3.70145248e-01,\n","       -1.91421272e-01, -5.08633319e-01, -2.69799412e-01,  3.22441883e-01,\n","       -1.62117390e-01, -2.44427936e-01,  4.49801538e-02, -1.60139037e-01,\n","       -6.26326836e-02, -7.06388549e-02, -1.53546791e-01, -1.95092858e-01,\n","       -2.82041414e-01, -2.65493195e-01, -5.08257775e-01, -1.05686639e+00,\n","       -4.60671508e-01, -2.99011981e-01, -1.12905839e+00, -7.10175407e-01,\n","       -4.73844150e-01, -8.33077151e-01, -2.20325663e-01, -2.00245520e-01,\n","       -1.25700116e-01, -4.89989340e-01, -7.14714523e-01, -6.31111562e-01,\n","       -6.48134080e-01, -3.96333912e-01, -1.12213465e+00, -7.93450294e-01,\n","       -1.31220727e+00, -1.26805904e+00, -8.71479987e-01, -8.67153042e-01,\n","       -8.38578526e-01, -8.88895368e-01, -4.80351579e-01, -8.34242682e-01,\n","       -8.67602616e-01, -7.35029295e-01, -1.18188874e-01, -4.62138543e-01,\n","       -7.05837913e-01, -3.84415815e-01, -1.95950712e-01, -1.83484438e-01,\n","       -6.98551606e-01,  1.43726316e+00,  2.99415957e-01,  5.94289654e-01,\n","        4.66266014e-01, -5.39560020e-02,  1.64395836e-01, -7.13054197e-02,\n","        1.75096423e-01,  1.96589658e-01, -2.18192830e-01, -6.33464561e-02,\n","       -2.60045428e-01,  2.14611584e-01, -1.63492422e-01,  6.82487231e-01,\n","       -7.68076079e-02,  3.22160853e-01,  9.50845907e-01,  1.31971019e+00,\n","        2.01251169e+00,  2.93725631e-02,  1.59674149e+00,  6.87422378e-01,\n","       -2.36316496e-01, -1.12486163e-01,  9.44668382e-01,  7.96382559e-01,\n","        1.80782593e+00,  1.38875085e+00,  1.05881402e+00,  1.97748750e+00,\n","        1.11199179e+00,  8.57902416e-01,  1.80618914e+00,  1.34131277e+00,\n","        1.72145641e+00,  1.42279128e+00,  1.69911835e+00,  3.19819038e-01,\n","        2.72705625e+00,  2.99547289e+00, -1.44577645e-01,  4.41015902e-02,\n","       -3.22788387e-01, -2.29013221e-01, -5.24521263e-01, -3.39089749e-01,\n","       -5.76957348e-01, -2.64542394e-01,  1.91616366e-01, -1.02587043e+00,\n","        6.64625626e-02, -1.59362100e-01,  3.35710634e-01, -1.91977337e-01,\n","        7.91327700e-02,  6.05511317e-01, -4.17482196e-01,  4.97392584e-01,\n","        5.56524534e-01,  2.70322624e+00,  2.76883818e+00,  1.44724378e+00,\n","        2.49863910e+00,  8.28173451e-01, -1.15431559e-01,  1.99004989e+00,\n","        2.65100998e+00,  2.71833944e+00,  6.18268768e-01, -5.91937150e-02,\n","        3.41028942e-01,  1.97388383e+00,  5.07476873e-01,  5.58098175e-01,\n","        3.30527701e-01, -1.00701744e-01,  1.94355036e-02,  4.89686735e-01,\n","       -3.07777743e-01, -4.71521356e-01,  2.28488323e-02,  4.22722947e-02,\n","        1.89660179e-01,  6.53340149e-01,  4.85410455e-01,  5.31775301e-01,\n","        1.14684027e+00,  2.12658763e+00,  1.07906083e-01, -1.38781278e-01,\n","        2.44253752e+00,  1.82778184e+00,  1.11931065e+00,  1.44684144e+00,\n","        2.16453888e+00,  2.47885783e+00,  1.43326618e+00,  1.59373912e+00,\n","        4.79552070e-02,  6.15205300e-01,  2.21684265e+00, -1.93806327e-01,\n","       -1.93854552e-01,  2.44665674e-01,  4.49427944e-01,  1.82511149e+00,\n","        1.15588118e+00,  1.34634384e+00,  1.62363627e+00,  1.20466188e+00,\n","        4.56053078e-01,  1.18080995e+00,  2.97075331e+00,  1.47693153e+00,\n","        2.77933680e+00,  1.30028649e+00,  3.96342764e-01, -1.06719467e-01,\n","        1.66072024e-01,  2.36025280e-01,  2.86469076e-01,  1.01476206e+00,\n","        1.32509520e+00,  6.81264065e-01,  6.65481386e-02, -6.11030881e-02,\n","        7.81966712e-01,  5.04752508e-01, -2.16601151e-01,  5.61478159e-01,\n","        1.60693175e+00,  1.18349991e+00,  3.25259220e-01,  3.24731993e-01,\n","        1.42891653e+00,  1.28627417e+00,  3.97015744e-01,  1.58768793e+00,\n","        7.70742475e-01,  8.36794948e-01, -8.30776356e-02, -4.92892054e-01,\n","        1.48263082e-01, -1.00895432e-01,  2.08845811e-01,  3.24729802e-01,\n","       -2.42866451e-01, -3.20334164e-01, -2.27879910e-01,  1.19083312e-01,\n","       -1.15869570e-01,  2.88580610e-01,  2.66656935e-01, -4.36795845e-03,\n","       -2.70164934e-01,  3.28010763e-01,  3.78523504e-01,  2.40654805e-01,\n","       -2.04935399e-01, -1.67265591e-01,  2.71691834e-01,  2.28143935e-02,\n","       -2.34823147e-01, -1.91010683e-02,  1.50968870e-01,  1.28322001e-01,\n","       -5.55113578e-02, -1.71553732e-01, -1.83644285e-01, -5.27338676e-02,\n","       -1.19567901e-01, -1.15189265e-01,  1.88261434e+00,  3.79476126e-01,\n","        6.94238968e-01,  1.23128675e+00, -7.40897223e-02, -1.85977610e-01,\n","        5.70002141e-01,  7.46672099e-01,  7.43405790e-01,  3.76467997e-01,\n","        7.03321465e-01, -1.16494154e-01,  1.03434524e+00, -2.26412145e-01,\n","       -2.31874948e-01, -7.45679498e-01, -2.64114243e-01, -1.45781556e-01,\n","       -2.32969440e-01,  1.48007986e-01, -3.60536528e-01, -2.92881760e-01,\n","       -4.39336699e-01,  6.45286861e-01, -3.92048991e-01,  1.37170779e-01,\n","       -1.34194014e+00, -8.96275085e-01, -1.79647747e-01, -1.30353392e+00,\n","       -1.15923480e+00, -1.36720239e+00, -1.20232208e+00, -7.80434155e-01,\n","       -1.17184926e+00, -1.27047691e+00, -1.35559206e+00, -1.44862852e+00,\n","       -1.62981560e+00, -1.51617640e+00, -1.54056277e+00, -1.55278577e+00,\n","       -9.96478082e-01, -6.56102171e-01, -8.50688095e-01, -1.41328688e+00,\n","       -4.64184470e-01, -5.81729357e-01, -7.38979257e-01, -9.88619532e-01,\n","       -9.32950858e-01, -1.66549575e+00, -1.69388148e+00, -1.56874905e+00,\n","       -1.07820770e+00, -1.08719517e+00, -8.93389432e-01, -1.56995030e+00,\n","       -1.23026441e+00, -1.13775473e+00, -3.30334052e-01, -1.51566592e+00,\n","       -1.04362546e+00, -2.22451182e-01, -1.17955071e+00, -1.31350106e+00,\n","       -9.17741458e-01, -1.06190062e+00, -1.52584465e+00, -1.31410368e+00,\n","       -1.49585937e+00, -1.03635813e+00, -1.22570816e+00, -4.68094510e-01,\n","       -5.16161739e-01, -4.26121878e-01, -1.32198179e+00, -6.63566349e-01,\n","       -1.38999207e+00, -5.22740019e-01, -3.94800796e-01, -1.17243362e+00,\n","       -1.39184033e+00, -7.77007653e-01, -1.03962685e+00, -1.35102248e-01,\n","       -6.20441941e-01, -4.64991624e-01, -1.30981358e+00, -8.49221224e-01,\n","       -1.54454110e+00, -1.58302832e+00, -1.21320690e+00, -1.16538763e+00,\n","       -1.00509081e+00, -6.28232674e-01, -9.45637548e-01, -1.33140422e+00,\n","       -1.37780606e+00, -7.93535669e-01, -6.15276117e-01, -8.02583087e-01,\n","       -9.84274580e-01, -8.18103539e-01, -8.43526035e-01, -7.22315101e-01,\n","       -5.38801298e-01, -9.56065835e-01, -8.70622326e-01, -8.66496039e-01,\n","       -6.41226718e-01, -5.97963573e-01, -4.15253778e-01, -6.90382861e-01,\n","       -4.19576311e-01, -3.40517461e-01,  3.63804047e-02, -2.65834452e-01,\n","       -4.09203321e-01, -6.63800616e-01, -1.11758828e+00, -7.65355166e-01,\n","       -4.61252156e-01, -5.87140360e-01, -2.31883633e-01, -3.91615248e-01,\n","       -4.72943340e-02, -7.48421403e-01, -1.39355396e+00, -9.26396586e-01,\n","       -1.36559644e+00, -7.90980654e-01, -2.53835625e-01, -6.54923909e-02,\n","        3.21486766e-01,  5.54879617e-01, -2.28801527e-01, -3.32898578e-01,\n","       -3.48223291e-02, -4.42362767e-01, -2.15897728e-01, -7.22623991e-01,\n","       -1.26162665e+00, -1.57465879e+00, -7.51578163e-01, -3.03423093e-01,\n","       -2.61905579e-01, -2.47832660e-01, -4.61898019e-01, -6.59717775e-01,\n","       -2.95451174e-01, -1.94009095e-01, -3.72471829e-01, -2.80738554e-01,\n","       -2.67925349e-03, -1.03144612e-01,  3.32070751e-01,  2.07545272e-01,\n","       -1.24322082e-01])"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["y_pred"]},{"cell_type":"code","execution_count":55,"id":"0b404547","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:21.945761Z","iopub.status.busy":"2022-04-27T11:25:21.945081Z","iopub.status.idle":"2022-04-27T11:25:21.948549Z","shell.execute_reply":"2022-04-27T11:25:21.949045Z","shell.execute_reply.started":"2022-04-09T12:29:13.058256Z"},"papermill":{"duration":0.063907,"end_time":"2022-04-27T11:25:21.949208","exception":false,"start_time":"2022-04-27T11:25:21.885301","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","execution_count":56,"id":"4cd477b1","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:22.073468Z","iopub.status.busy":"2022-04-27T11:25:22.072835Z","iopub.status.idle":"2022-04-27T11:25:22.075434Z","shell.execute_reply":"2022-04-27T11:25:22.074872Z","shell.execute_reply.started":"2022-04-09T12:30:10.985068Z"},"papermill":{"duration":0.065266,"end_time":"2022-04-27T11:25:22.07559","exception":false,"start_time":"2022-04-27T11:25:22.010324","status":"completed"},"tags":[]},"outputs":[],"source":["mse=mean_squared_error(svr_y,z)"]},{"cell_type":"code","execution_count":57,"id":"3b6b4a68","metadata":{"execution":{"iopub.execute_input":"2022-04-27T11:25:22.19445Z","iopub.status.busy":"2022-04-27T11:25:22.193897Z","iopub.status.idle":"2022-04-27T11:25:22.196704Z","shell.execute_reply":"2022-04-27T11:25:22.197139Z","shell.execute_reply.started":"2022-04-09T12:30:14.016056Z"},"papermill":{"duration":0.064224,"end_time":"2022-04-27T11:25:22.197299","exception":false,"start_time":"2022-04-27T11:25:22.133075","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["3749686377.1463037"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["mse"]},{"cell_type":"markdown","id":"cd71bebb","metadata":{"papermill":{"duration":0.056329,"end_time":"2022-04-27T11:25:22.310368","exception":false,"start_time":"2022-04-27T11:25:22.254039","status":"completed"},"tags":[]},"source":["WOrking with decision tree regression in next notebook."]},{"cell_type":"code","execution_count":null,"id":"54a3054c","metadata":{"papermill":{"duration":0.056999,"end_time":"2022-04-27T11:25:22.424796","exception":false,"start_time":"2022-04-27T11:25:22.367797","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":18.951356,"end_time":"2022-04-27T11:25:23.191136","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-04-27T11:25:04.23978","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}